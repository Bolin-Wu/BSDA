\documentclass[10pt,handout]{beamer}
%\documentclass[10pt]{beamer}
\usepackage[english]{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\usetheme{Uppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}

% Input new commands
\input{../common/commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Bayesian Statistics and Data Analysis \\ Lecture 1}}
\author[]{M{\aa}ns Magnusson \\ Department of Statistics, Uppsala University}
\date{}

\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
\frame{\sectionpage}

\begin{frame}

  \frametitle{Decision making in case of uncertainties}

  \begin{center}
    \includegraphics[width=10.5cm]{figs/irma.png}
  \end{center}
\end{frame}


\begin{frame}{Bayesian Analysis}

  \begin{itemize}
  \item Based on Bayesian probability theory
    \begin{itemize}
    \item uncertainty is presented with probabilities
    \item probabilities are updated based on new information
    % \item {\it ...common sense reduced to calculation},
    %   Laplace 1819
    \end{itemize}
    \pause
    \item Thomas Bayes (170?--1761)
    \begin{itemize}
    \item English nonconformist, Presbyterian minister,
      mathematician
    % \item Richard Price published Bayes' paper on conditional
    %   probabilities in 1763 after Bayes had died
    \item considered the problem of {\it inverse probability}
      \begin{itemize}
        \item significant part of the Bayesian theory
      \end{itemize}
  \end{itemize}
  \pause
  \item Bayes did not invent all, but was first to solve problem of
    inverse probability in special case
  \item Modern Bayesian theory with rigorous proofs developed in
    20th century
\end{itemize}
\end{frame}

\begin{frame}{Term Bayesian used first time in mid 20th century}

  \begin{itemize}
  \item Earlier there was just "probability theory"
    \begin{itemize}
    \item concept of the probability was not strictly defined,
      although it was close to modern Bayesian interpretation
    \item in the end of 19th century there were increasing demand
      for more strict definition of probability (mathematical and
      philosophical problem)
    \end{itemize}
    \pause
    \item In the beginning of 20th century frequentist view gained popularity
    \begin{itemize}
    \item accepts definition of probabilities only through frequencies
    \item does not accept inverse probability or use of prior
    \item gained popularity due to apparent objectivity and "cook
      book" like reference books
    \end{itemize}
    \pause
    \item R. A. Fisher used in 1950 first
      time term "Bayesian" to emphasize the difference to general
      term "probability theory"
      \begin{itemize}
      \item term became quickly popular, because alternative
        descriptions were longer
    %     \pause
    % \item after this Bayesians started to use term "frequentist"
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Uncertainty and probabilistic modeling}

  \begin{itemize}
  \item Two types of uncertainty: aleatoric and epistemic
    \vspace{\baselineskip}
  \item Representing uncertainty with probabilities
    \vspace{\baselineskip}
  \item Updating uncertainty
   \end{itemize}
\end{frame}


\begin{frame}{Two types of uncertainty}

  \begin{itemize}
  \item Aleatoric uncertainty due to randomness
    \begin{itemize}
    \item<2-> we are not able to obtain observations which could reduce
      this uncertainty
    \end{itemize}
    \vspace{\baselineskip}
  \item Epistemic uncertainty due to lack of knowledge
    \begin{itemize}
    \item<3-> we are able to obtain observations which can reduce
      this uncertainty
    \item<3-> two observers may have different epistemic uncertainty
    %\item<3-> epistemic uncertainty chances when the information chances
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}{Updating uncertainty}

  \begin{itemize}
  %\item<2-> probability of red $\frac{\mathrm{\#red}}{\mathrm{\#red+\#yellow}}{\onslide<3->=\theta}$
  \item<2-> Probability of red $\frac{\mathrm{\#red}}{\mathrm{\#red+\#yellow}}=\theta$
    \vspace{\baselineskip}
  \item<3-> $p(y=\mathrm{red}|\theta)=\theta \quad$ aleatoric uncertainty
    \vspace{\baselineskip}
  \item<4-> $p(\theta) \quad$ epistemic uncertainty
    \vspace{\baselineskip}
  \item<5-> Picking many chips updates our uncertainty about the proportion
    \vspace{\baselineskip}
  \item<5-> $p(\theta|\mathrm{y=red,yellow,red,red,\ldots})=?$
    \vspace{\baselineskip}
  \item<6-> Bayes rule
      $p(\theta|y)=\frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta}$
  \end{itemize}
\end{frame}


\begin{frame}{Model vs. likelihood}

  \begin{itemize}
  \item Bayes rule
      $p(\theta|y)\propto p(y|\theta)p(\theta)$
    \vspace{\baselineskip}
  \item Model: $p(y|\theta)$ as a function of $y$ given fixed $\theta$
    describes the aleatoric uncertainty \vspace{\baselineskip}
  \item Likelihood: $p(y|\theta)$ %=L(\theta|y)$
    as a function of $\theta$
    given fixed $y$ provides information about epistemic uncertainty,
    but is not a probability distribution
    \vspace{\baselineskip}
  \item<2-> Bayes rule combines the likelihood with prior uncertainty
    $p(\theta)$ and transforms them to updated posterior uncertainty
  \end{itemize}
\end{frame}

%% TODO: Change example

\begin{frame}{Example application: Drug dosage for liver transplant\footnote{\color{gray}with E. Siivola, Aalto and S. Weber, Novartis Pharma}}

  \vspace{-0.5\baselineskip}
\begin{itemize}
\item Everolimus is immunosuppressant to prevent rejection of organ
  transplants
\item Pharmacokinetic model of drug and body, optimal dosage depends on weight\\
  \begin{minipage}[t]{\textwidth}
    \vspace{-.2\baselineskip}
  \hspace{-1.2cm}\includegraphics[width=6cm]{figs/2compartment_graph.png}
  \includegraphics[width=6cm]{figs/data_population_simple.pdf}
\end{minipage}
    \vspace{.2\baselineskip}
\item<2-> Model fitted with 500 adults, extrapolation to children?
\item<3-> Maturation effect, 17 observations from children
\end{itemize}

\end{frame}

\begin{frame}{The art of probabilistic modeling}

  \begin{itemize}
  \item The art of probabilistic modeling is to describe in a
    mathematical form (model and prior distributions) what we already
    know and what we don't know
\vspace{\baselineskip}
  \item<2-> ``Easy'' part is to use Bayes rule to update the uncertainties
    \begin{itemize}
    \item computational challenges
    \end{itemize}
\vspace{\baselineskip}
  \item<3-> Other parts of the art of probabilistic modeling are, for example,
    \begin{itemize}
    \item model checking: is data in conflict with our prior knowledge?
    \item presentation: presenting the model and the results to the application experts
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}

  \begin{columns}[T] % align columns
 \begin{column}{.49\textwidth}
{  \footnotesize
  \begin{itemize}
 \item Galaxy clusters for cosmology
 \item Coagulation of blood
 \item Gene regulation
 \item Pharmacokinetics and -dynamics
 \item Decision support
 \item Effects of nutrition for diabetes
 \item Evolutionary anthropology
 \item Clinical trial designs
 \item Daily demand for gas
 \item Brain structure trees
 \item School enrollment
 \item Sports
 \item Product demand
 \end{itemize}
 }
\end{column}
 \begin{column}{.49\textwidth}
{    \footnotesize
  \begin{itemize}
 \item Cocoa bean fermentation
 \item Marine propulsion power
 \item Alcohol consumption trends
 \item Flood probability
 \item Instantaneous heart rate distributions
 \item Drug dosing regimens in pediatrics
 \item Human T stem cell memory cells
 \item Fairness in university admission policies
 \item Destruction of bacteria and bacterial spores under heat
 \end{itemize}
 }
\end{column}
\end{columns}
\end{frame}

\begin{frame}
  \frametitle{Bayesian data analysis}  %
  \framesubtitle{Example analyses}
  \begin{itemize}
  \item Treatment/control
    \begin{itemize}
    \item randomize patients to treatment or control
    \item is the treatment effective?
    \end{itemize}
    \pause
  \item Continuous valued treatment
    \begin{itemize}
    \item randomize patients with different dosages
    \item which dosage is sufficient without too many side effects?
    \end{itemize}
    \pause
  \item Different effects for different patients?
    \begin{itemize}
    \item Is the treatment effect different for male/female, child/adult, light/heavy, ...
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Bayesian approach}

  \begin{itemize}
  \item Benefits of Bayesian approach
    \begin{itemize}
    \item integrate over uncertainties to focus to interesting parts
    \item use relevant prior information
    \item hierarchical models
    \item model checking and evaluation
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Computation}

  We need to be able to compute expectations with respect to posterior
  distribution $p(\theta|y)$
  \begin{align*}
    \E_{\theta|y}\left[ g(\theta) \right] = \int p(\theta|y)g(\theta) d\theta
  \end{align*}

  \begin{itemize}
  \item Analytic
    \begin{itemize}
    \item only for very simple models
    \end{itemize}
  \item Monte Carlo, Markov chain Monte Carlo
    \begin{itemize}
    \item generic
    \end{itemize}
  \item Distributional approximations
    \begin{itemize}
    \item e.g. Laplace, variational, expectation propagation
    \item less generic, but can be much faster with sufficient accuracy
    \end{itemize}
  \end{itemize}
\end{frame}

\end{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
